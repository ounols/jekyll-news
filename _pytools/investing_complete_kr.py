#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Investing.com ì™„ì „íŒ í¬ë¡¤ëŸ¬ (í•œêµ­ì–´)
- APIë¡œ Breaking News ëª©ë¡ + ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
- ì‹¤ì œ í˜ì´ì§€ì—ì„œ ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§
- ê´€ë ¨ ì£¼ì‹ ì •ë³´ ì¶”ê°€
- í•œêµ­ì–´ ìë™ ë²ˆì—­ (í•„ìš”ì‹œ)
"""

import cloudscraper
from bs4 import BeautifulSoup
from readability import Document
from deep_translator import GoogleTranslator
from datetime import datetime
import re
from pathlib import Path
import sys
import time

# Windows ì½˜ì†” ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')


class InvestingCompleteKR:
    def __init__(self):
        self.base_url = "https://www.investing.com"
        self.api_url = "https://endpoints.investing.com/news-delivery/api/v2/articles/delivery/domains/18/news/lists/breaking-news"
        self.instrument_api_url = "https://endpoints.investing.com/pd-instruments/v1/instruments"
        self.posts_dir = Path(__file__).parent.parent / "_posts"
        self.posts_dir.mkdir(exist_ok=True)
        
        # cloudscraper ì„¸ì…˜ ìƒì„±
        self.scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            }
        )
        
        # ë²ˆì—­ê¸° ì´ˆê¸°í™”
        self.translator = GoogleTranslator(source='en', target='ko')
        self.bearer_token = None
    
    def search_instrument(self, search_text):
        """
        Investing.com ê²€ìƒ‰ APIë¡œ ì¢…ëª© ì •ë³´ ì¡°íšŒ

        Args:
            search_text: ê²€ìƒ‰ì–´ (ì˜ˆ: "nvda", "005930", "samsung")

        Returns:
            dict: {'id': int, 'symbol': str, 'name': str, 'aql_link': str, 'exchange': str}
            ë˜ëŠ” None (ê²€ìƒ‰ ì‹¤íŒ¨)
        """
        if not search_text:
            return None

        try:
            url = "https://kr.investing.com/search/service/search"

            data = {
                'search_text': search_text,
                'term': search_text,
                'country_id': '0',
                'tab_id': 'All'
            }

            headers = {
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br, zstd',
                'Content-Type': 'application/x-www-form-urlencoded',
                'X-Requested-With': 'XMLHttpRequest',
                'Origin': 'https://kr.investing.com',
                'Referer': 'https://kr.investing.com/',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:145.0) Gecko/20100101 Firefox/145.0'
            }

            response = self.scraper.post(url, data=data, headers=headers, timeout=30)

            if response.status_code == 200:
                result_data = response.json()
                all_results = result_data.get('All', [])

                if all_results and len(all_results) > 0:
                    first_result = all_results[0]

                    inst_info = {
                        'id': first_result.get('pair_ID'),
                        'symbol': first_result.get('symbol'),
                        'name': first_result.get('name'),
                        'aql_link': first_result.get('aql_link'),
                        'exchange': first_result.get('exchange_popular_symbol')
                    }

                    return inst_info

            return None

        except Exception as e:
            print(f"[WARNING] ì¢…ëª© ê²€ìƒ‰ ì‹¤íŒ¨ ({search_text}): {e}")
            return None

    def extract_bearer_token(self):
        """Bearer í† í° ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            print("[INFO] Bearer í† í° ì¶”ì¶œ ì¤‘...")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
            }
            response = self.scraper.get(f"{self.base_url}/news/latest-news", headers=headers, timeout=30)

            if response.status_code != 200:
                print(f"[WARNING] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ (HTTP {response.status_code})")

            # ë°©ë²• 1: __NEXT_DATA__ì—ì„œ accessToken ì¶”ì¶œ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
            soup = BeautifulSoup(response.text, 'lxml')
            next_data = soup.find('script', {'id': '__NEXT_DATA__'})

            if next_data:
                try:
                    import json
                    data = json.loads(next_data.string)

                    # props.pageProps.accessToken ê²½ë¡œë¡œ ì ‘ê·¼
                    access_token = data.get('props', {}).get('pageProps', {}).get('accessToken')

                    if access_token and len(access_token) > 100 and '.' in access_token:
                        print(f"[OK] __NEXT_DATA__ì—ì„œ JWT í† í° ë°œê²¬ (ê¸¸ì´: {len(access_token)})")
                        return access_token
                except Exception as e:
                    print(f"[WARNING] __NEXT_DATA__ íŒŒì‹± ì‹¤íŒ¨: {e}")

            # ë°©ë²• 2: Regex íŒ¨í„´ ì‚¬ìš© (Fallback)
            print("[INFO] Regex íŒ¨í„´ìœ¼ë¡œ ì‹œë„ ì¤‘...")

            patterns = [
                r'"accessToken"\s*:\s*"([^"]+)"',
                r'eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+',
                r'"token"\s*:\s*"(eyJ[^"]+)"',
                r'Bearer\s+([A-Za-z0-9\-_\.]+)',
                r'token["\']?\s*[:=]\s*["\']([A-Za-z0-9\-_\.]+)',
            ]

            all_tokens = []
            for pattern in patterns:
                matches = re.findall(pattern, response.text, re.IGNORECASE)
                all_tokens.extend(matches)

            # JWT í† í° ì°¾ê¸° (ë³´í†µ 100ì ì´ìƒ, '.'ì´ í¬í•¨ë¨)
            jwt_tokens = [t for t in all_tokens if len(t) > 100 and '.' in t]

            if jwt_tokens:
                token = jwt_tokens[0]
                print(f"[OK] Regexë¡œ JWT í† í° ë°œê²¬ (ê¸¸ì´: {len(token)})")
                return token

            # JWTë¥¼ ëª» ì°¾ìœ¼ë©´ ê°€ì¥ ê¸´ í† í° (100ì ì´ìƒ)
            long_tokens = [t for t in all_tokens if len(t) > 100]
            if long_tokens:
                token = long_tokens[0]
                print(f"[OK] í† í° ë°œê²¬ (ê¸¸ì´: {len(token)})")
                return token

            print("[WARNING] í† í° ì—†ì´ ì§„í–‰")
            return None

        except Exception as e:
            print(f"[WARNING] í† í° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def fetch_breaking_news_api(self):
        """APIë¡œ Breaking News ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if not self.bearer_token:
                self.bearer_token = self.extract_bearer_token()
            
            print(f"\n[INFO] API í˜¸ì¶œ ì¤‘...")

            headers = {
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'en-US,en;q=0.9,ko;q=0.8',
                'Origin': 'https://www.investing.com',
                'Referer': 'https://www.investing.com/',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            }

            # Bearer í† í°ì´ ìˆì„ ë•Œë§Œ Authorization í—¤ë” ì¶”ê°€
            if self.bearer_token:
                headers['Authorization'] = f'Bearer {self.bearer_token}'

            response = self.scraper.get(self.api_url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])
                print(f"[OK] APIë¡œë¶€í„° {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì‹ \n")
                
                # ê¸°ì‚¬ ì •ë³´ íŒŒì‹±
                parsed_articles = []
                for article in articles:
                    # ë©”ì¸ ì´ë¯¸ì§€ ì¶”ì¶œ
                    main_image = None
                    media = article.get('media', [])
                    for m in media:
                        if m.get('purpose') == 'main_image':
                            main_image = m.get('url')
                            break
                    
                    # URL ìƒì„±
                    link = article.get('link', '')
                    full_url = f"{self.base_url}{link}" if link.startswith('/') else link
                    
                    # ê´€ë ¨ ì£¼ì‹ ID ì¶”ì¶œ
                    instruments = article.get('instruments') or []
                    instrument_ids = [inst['id'] for inst in instruments if inst and inst.get('primary_tag')]
                    
                    parsed_articles.append({
                        'id': article.get('id'),
                        'title': article.get('title', ''),
                        'url': full_url,
                        'summary': article.get('body', ''),  # API ìš”ì•½
                        'image_url': main_image,
                        'instrument_ids': instrument_ids[:5],  # ìµœëŒ€ 5ê°œë§Œ
                        'published': article.get('published_at', ''),
                    })
                
                return parsed_articles
            else:
                print(f"[ERROR] API í˜¸ì¶œ ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code})")
                return None
                
        except Exception as e:
            print(f"[ERROR] API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def fetch_full_article_content(self, url):
        """ì‹¤ì œ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§"""
        try:
            print(f"  - ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§ ì¤‘...")
            time.sleep(1)

            # í•œêµ­ì–´ ì‚¬ì´íŠ¸ì™€ ì˜ì–´ ì‚¬ì´íŠ¸ ëª¨ë‘ ì‹œë„
            urls_to_try = [
                url.replace('www.investing.com', 'kr.investing.com'),  # í•œêµ­ì–´ ìš°ì„ 
                url  # ì˜ì–´ ì›ë³¸
            ]

            for try_url in urls_to_try:
                try:
                    response = self.scraper.get(try_url, timeout=30)
                    response.raise_for_status()

                    soup = BeautifulSoup(response.text, 'lxml')

                    # __NEXT_DATA__ì—ì„œ ì „ì²´ ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ
                    next_data_script = soup.find('script', {'id': '__NEXT_DATA__'})

                    if next_data_script:
                        try:
                            import json
                            data = json.loads(next_data_script.string)

                            # í˜„ì¬ ê¸°ì‚¬ì˜ ë³¸ë¬¸ì„ ì°¾ê¸° ìœ„í•´ articleStoreë¥¼ ìš°ì„  íƒìƒ‰
                            def find_article_body(obj, depth=0):
                                if depth > 10:
                                    return None

                                if isinstance(obj, dict):
                                    # articleStoreë‚˜ article í‚¤ë¥¼ ì°¾ì•„ì„œ ê·¸ ì•ˆì˜ bodyë¥¼ ìš°ì„ 
                                    if 'articleStore' in obj or 'article' in obj:
                                        article_obj = obj.get('articleStore') or obj.get('article')
                                        if isinstance(article_obj, dict):
                                            body = article_obj.get('body', '')
                                            if isinstance(body, str) and len(body) > 500:
                                                return {'title': article_obj.get('title', ''), 'body': body}

                                    # ì¼ë°˜ body ê²€ìƒ‰ (ê¸¸ì´ê°€ ì¶©ë¶„íˆ ê¸´ ê²ƒë§Œ)
                                    if 'body' in obj and 'title' in obj:
                                        body = obj.get('body', '')
                                        if isinstance(body, str) and len(body) > 500:
                                            return {'title': obj.get('title', ''), 'body': body}

                                    # ì¬ê·€ íƒìƒ‰
                                    for v in obj.values():
                                        result = find_article_body(v, depth+1)
                                        if result:
                                            return result

                                elif isinstance(obj, list):
                                    for item in obj:
                                        result = find_article_body(item, depth+1)
                                        if result:
                                            return result

                                return None

                            article_data = find_article_body(data)

                            if article_data and article_data.get('body'):
                                title = article_data.get('title', '')
                                body_html = article_data['body']

                                # HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                                body_soup = BeautifulSoup(body_html, 'lxml')

                                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                paragraphs = body_soup.find_all(['p', 'h2', 'h3', 'li'])
                                content_parts = []

                                for p in paragraphs:
                                    text = p.get_text().strip()
                                    # ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ë§Œ
                                    if text and len(text) > 15:
                                        # ê´‘ê³ ì„± ë¬¸êµ¬ í•„í„°ë§
                                        skip_phrases = ['subscribe', 'newsletter', 'sign up', 'click here',
                                                       'êµ¬ë…', 'ë‰´ìŠ¤ë ˆí„°', 'ê°€ì…']
                                        if not any(skip.lower() in text.lower() for skip in skip_phrases):
                                            content_parts.append(text)

                                content = '\n\n'.join(content_parts)

                                # ë³¸ë¬¸ ê²€ì¦
                                if content and self.is_valid_article_content(content):
                                    print(f"  - __NEXT_DATA__ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(content)} ì)")
                                    return title, content
                                elif content:
                                    print(f"  - __NEXT_DATA__ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ (ë²•ì  ê³ ì§€ ë“±)")
                                    # ë‹¤ìŒ URL ì‹œë„
                                    continue

                        except Exception as e:
                            print(f"  [WARNING] __NEXT_DATA__ íŒŒì‹± ì‹¤íŒ¨: {e}")

                    # ì´ URLì—ì„œ ì„±ê³µí–ˆë‹¤ë©´ Readability ì‹œë„í•˜ì§€ ì•Šê³  ë‹¤ìŒ URLë¡œ
                    # (í•œêµ­ì–´ì—ì„œ ì‹¤íŒ¨í•˜ë©´ ì˜ì–´ ì‹œë„)

                except Exception as e:
                    # 404ë‚˜ ë‹¤ë¥¸ ì—ëŸ¬ë©´ ë‹¤ìŒ URL ì‹œë„
                    if '404' in str(e):
                        print(f"  - í•œêµ­ì–´ í˜ì´ì§€ ì—†ìŒ, ì˜ì–´ í˜ì´ì§€ ì‹œë„...")
                        continue
                    print(f"  [WARNING] í¬ë¡¤ë§ ì‹¤íŒ¨ ({try_url}): {e}")
                    continue
            
            # Fallback: Readability ì‚¬ìš©
            print(f"  - Readability ë°©ì‹ìœ¼ë¡œ ì‹œë„...")
            doc = Document(response.text)
            title = doc.title()
            content_html = doc.summary()

            soup2 = BeautifulSoup(content_html, 'lxml')
            paragraphs = soup2.find_all(['p', 'h2', 'h3'])
            content_parts = []

            for p in paragraphs:
                text = p.get_text().strip()
                if text and len(text) > 20:
                    content_parts.append(text)

            content = '\n\n'.join(content_parts)

            # Readability ê²°ê³¼ ê²€ì¦
            if content and self.is_valid_article_content(content):
                print(f"  - Readabilityë¡œ ë³¸ë¬¸ ì¶”ì¶œ ({len(content)} ì)")
                return title, content
            elif content:
                print(f"  - Readability ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ (ë²•ì  ê³ ì§€ ë“±)")

            return None, None
            
        except Exception as e:
            print(f"  [WARNING] ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None, None
    
    def fetch_instrument_info(self, instrument_ids):
        """ê´€ë ¨ ì£¼ì‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if not instrument_ids:
            return []
        
        try:
            print(f"  - ê´€ë ¨ ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì¤‘ ({len(instrument_ids)}ê°œ)...")
            
            instruments_info = []
            # ê° ì£¼ì‹ IDë¥¼ ê°œë³„ì ìœ¼ë¡œ ì¡°íšŒ
            for inst_id in instrument_ids[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                try:
                    url = f"{self.instrument_api_url}?instrument_ids={inst_id}"
                    response = self.scraper.get(url, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        if data and len(data) > 0:
                            inst = data[0]
                            instruments_info.append({
                                'id': inst.get('id'),
                                'name': inst.get('long_name', inst.get('short_name', '')),
                                'symbol': inst.get('symbol', ''),
                                'exchange_id': inst.get('exchange_id'),
                                'price': inst.get('price', {}),
                                'link': f"{self.base_url}{inst.get('link', '')}" if inst.get('link') else '',
                            })
                    
                    time.sleep(0.3)  # Rate limiting
                except:
                    continue
            
            print(f"  - ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì™„ë£Œ ({len(instruments_info)}ê°œ)")
            return instruments_info
            
        except Exception as e:
            print(f"  [WARNING] ì£¼ì‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def is_valid_article_content(self, text):
        """ë³¸ë¬¸ì´ ìœ íš¨í•œ ê¸°ì‚¬ ë‚´ìš©ì¸ì§€ ê²€ì¦"""
        if not text or len(text) < 200:
            return False

        # ë²•ì  ê³ ì§€ì‚¬í•­ì´ë‚˜ ë¶ˆí•„ìš”í•œ ë‚´ìš© í•„í„°ë§
        invalid_keywords = [
            'risk warning', 'disclaimer', 'ë¦¬ìŠ¤í¬ ê³ ì§€', 'ë©´ì±… ì¡°í•­',
            'fusion media', 'íŒê¶Œì†Œìœ ', 'all rights reserved',
            'terms and conditions', 'ì´ìš©ì•½ê´€',
            'privacy policy', 'ê°œì¸ì •ë³´ ë³´í˜¸ì •ì±…'
        ]

        # í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ 500ìë¥¼ ê²€ì‚¬ (ë²•ì  ê³ ì§€ê°€ ì•ì— ì˜¤ëŠ” ê²½ìš°ê°€ ë§ìŒ)
        text_start = text[:500].lower()

        # ì—¬ëŸ¬ ê°œì˜ invalid í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë²•ì  ê³ ì§€ë¡œ íŒë‹¨
        keyword_count = sum(1 for keyword in invalid_keywords if keyword.lower() in text_start)

        if keyword_count >= 2:
            print(f"  [WARNING] ë²•ì  ê³ ì§€ì‚¬í•­ìœ¼ë¡œ íŒë‹¨ë˜ì–´ ìŠ¤í‚µ (í‚¤ì›Œë“œ {keyword_count}ê°œ ë°œê²¬)")
            return False

        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë²•ì  í‚¤ì›Œë“œ ë¹„ìœ¨ í™•ì¸
        total_text_lower = text.lower()
        legal_word_count = sum(total_text_lower.count(keyword.lower()) for keyword in invalid_keywords)

        # í…ìŠ¤íŠ¸ê°€ ì§§ì€ë° ë²•ì  í‚¤ì›Œë“œê°€ ë§ìœ¼ë©´ ì˜ì‹¬
        if len(text) < 1000 and legal_word_count >= 5:
            print(f"  [WARNING] ë²•ì  ê³ ì§€ì‚¬í•­ ë¹„ìœ¨ì´ ë†’ì•„ ìŠ¤í‚µ")
            return False

        return True

    def is_korean(self, text):
        """í…ìŠ¤íŠ¸ê°€ í•œêµ­ì–´ì¸ì§€ í™•ì¸"""
        if not text:
            return False
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.sub(r'\s', '', text))
        if total_chars == 0:
            return False
        return (korean_chars / total_chars) > 0.3
    
    def translate_to_korean(self, text, max_length=4500):
        """í•œêµ­ì–´ë¡œ ë²ˆì—­"""
        try:
            if not text or len(text.strip()) == 0:
                return text
            
            if self.is_korean(text):
                return text
            
            if len(text) > max_length:
                text = text[:max_length]
            
            translated = self.translator.translate(text)
            time.sleep(0.5)
            return translated
            
        except Exception as e:
            print(f"  [WARNING] ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text
    
    def sanitize_filename(self, text):
        """íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        text = re.sub(r'[^\w\sã„±-ã…ã…-ã…£ê°€-í£-]', '', text)
        text = re.sub(r'[-\s]+', '-', text)
        return text.strip('-')[:80]
    
    def clean_title(self, title):
        """ì œëª©ì—ì„œ ì¶œì²˜ ì •ë³´ ì œê±°"""
        if not title:
            return title
        
        # "By Investing.com", "By InvestingPro" ë“± íŒ¨í„´ ì œê±°
        patterns = [
            r'\s*By\s+Investing\.com\s*$',
            r'\s*By\s+InvestingPro\s*$',
            r'\s*-\s*Investing\.com\s*$',
            r'\s*-\s*InvestingPro\s*$',
        ]
        
        cleaned = title
        for pattern in patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        return cleaned.strip()
    
    def convert_tickers_to_badges(self, text, instruments_info=None):
        """
        í‹°ì»¤ ì‹¬ë³¼ì„ ì‹¤ì‹œê°„ ë±ƒì§€ë¡œ ë³€í™˜
        API ë°ì´í„° ë˜ëŠ” ë™ì  ê²€ìƒ‰ìœ¼ë¡œ instrument_id ì¡°íšŒ
        """
        # íŒ¨í„´: (KS:005930), (NASDAQ:NVDA), (TYO:9984) ë“±
        pattern = r'\(([A-Z]+):([A-Z0-9]+)\)'

        # instrument ì •ë³´ë¡œë¶€í„° symbol -> id ë§¤í•‘ ìƒì„±
        symbol_to_id = {}
        if instruments_info:
            for inst in instruments_info:
                symbol = inst.get('symbol')
                inst_id = inst.get('id')
                if symbol and inst_id:
                    symbol_to_id[symbol] = inst_id

        def replace_ticker(match):
            exchange = match.group(1)
            symbol = match.group(2)
            full_ticker = f"{exchange}:{symbol}"

            # 1. API ë°ì´í„°ì—ì„œ ë¨¼ì € ì¡°íšŒ
            instrument_id = symbol_to_id.get(symbol)

            # 2. ì—†ìœ¼ë©´ ê²€ìƒ‰ APIë¡œ ë™ì  ì¡°íšŒ
            if not instrument_id:
                search_result = self.search_instrument(symbol)
                if search_result and search_result.get('id'):
                    instrument_id = search_result['id']

            # 3. ì°¾ì§€ ëª»í•˜ë©´ í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ì œê±°
            if not instrument_id:
                return ''

            # HTML ë§ˆí¬ì—…ìœ¼ë¡œ ë³€í™˜
            return f'<span class="stock-ticker" data-ticker="{full_ticker}" data-exchange="{exchange}" data-symbol="{symbol}" data-instrument-id="{instrument_id}">({full_ticker})</span>'

        return re.sub(pattern, replace_ticker, text)
    
    def create_post(self, article, index):
        """ì™„ì „í•œ Jekyll í¬ìŠ¤íŠ¸ ìƒì„±"""
        try:
            original_title = article.get('title', '').strip()
            article_url = article.get('url', '')
            image_url = article.get('image_url', '')
            
            if not original_title or not article_url:
                return False
            
            print(f"\n{'='*70}")
            print(f"[{index}] {original_title[:50]}...")
            print(f"{'='*70}")
            
            # 1. ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            # API ìš”ì•½ì„ ë¨¼ì € ì‚¬ìš©í•˜ê³ , í•„ìš”ì‹œ í¬ë¡¤ë§ ì‹œë„
            summary_content = article.get('summary', '')
            full_title = None
            full_content = summary_content
            
            # ìš”ì•½ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ í¬ë¡¤ë§ ì‹œë„
            if len(summary_content) < 100:
                print(f"  - ìš”ì•½ì´ ì§§ì•„ ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œë„...")
                full_title, crawled_content = self.fetch_full_article_content(article_url)
                if crawled_content and len(crawled_content) > len(summary_content):
                    full_content = crawled_content
                    print(f"  - í¬ë¡¤ë§ ì„±ê³µ ({len(crawled_content)} ì)")
                else:
                    print(f"  - í¬ë¡¤ë§ ì‹¤íŒ¨, API ìš”ì•½ ì‚¬ìš©")
            else:
                print(f"  - API ìš”ì•½ ì‚¬ìš© ({len(summary_content)} ì)")
            
            if not full_content or len(full_content) < 50:
                print(f"  [SKIP] ì¶©ë¶„í•œ ë³¸ë¬¸ì´ ì—†ìŒ")
                return False
            
            # 2. ê´€ë ¨ ì£¼ì‹ ì •ë³´
            instruments = self.fetch_instrument_info(article.get('instrument_ids', []))
            
            # 3. ì œëª© ê²°ì • ë° ë²ˆì—­
            title_to_use = full_title if full_title else original_title
            
            print(f"  - ì œëª© ë²ˆì—­ ì¤‘...")
            title_kr = self.translate_to_korean(title_to_use)
            
            # ì œëª©ì—ì„œ ì¶œì²˜ ì •ë³´ ì œê±°
            title_kr = self.clean_title(title_kr)
            
            # 4. ë³¸ë¬¸ ë²ˆì—­
            print(f"  - ë³¸ë¬¸ ë²ˆì—­ ì¤‘...")
            content_chunks = [full_content[i:i+4500] for i in range(0, len(full_content), 4500)]
            content_kr_parts = []
            
            for i, chunk in enumerate(content_chunks[:3], 1):
                if i > 1:
                    print(f"  - ë²ˆì—­ ì§„í–‰ ì¤‘... ({i}/{min(3, len(content_chunks))})")
                translated = self.translate_to_korean(chunk)
                if translated:
                    content_kr_parts.append(translated)
            
            content_kr = '\n\n'.join(content_kr_parts)
            
            # 5. ìš”ì•½ ìƒì„± (í‹°ì»¤ ë³€í™˜ ì „, ì¢…ëª© ì½”ë“œ ì œê±°)
            excerpt_text = content_kr[:200] if len(content_kr) > 200 else content_kr
            # ì¢…ëª© ì½”ë“œ íŒ¨í„´ ì œê±° (ì˜ˆ: (KS:005930), (NASDAQ:NVDA))
            excerpt_clean = re.sub(r'\([A-Z]+:[A-Z0-9]+\)', '', excerpt_text)
            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            excerpt_clean = re.sub(r'\s+', ' ', excerpt_clean).strip()
            excerpt = excerpt_clean + "..." if len(content_kr) > 200 else excerpt_clean
            
            # 5.5. í‹°ì»¤ ì‹¬ë³¼ì„ ì‹¤ì‹œê°„ ë±ƒì§€ë¡œ ë³€í™˜ (excerpt ìƒì„± í›„)
            content_kr = self.convert_tickers_to_badges(content_kr, instruments)
            
            # 6. ì£¼ì‹ ì •ë³´ ë§ˆí¬ë‹¤ìš´ ìƒì„± (front matterì— í¬í•¨ë˜ê¸° ë•Œë¬¸ì— ë³¸ë¬¸ì€ ìƒëµ)
            # instruments_md = ""
            # if instruments:
            #     instruments_md = "\n\n## ğŸ“ˆ ê´€ë ¨ ì£¼ì‹\n\n"
            #     for inst in instruments:
            #         name = inst.get('name', '')
            #         symbol = inst.get('symbol', '')
            #         price_info = inst.get('price', {})
            #         link = inst.get('link', '')
                    
            #         last_price = price_info.get('last', 0)
            #         change = price_info.get('change', 0)
            #         change_percent = price_info.get('change_percent', 0)
                    
            #         # ë“±ë½ ì•„ì´ì½˜
            #         icon = "ğŸ”º" if change > 0 else "ğŸ”»" if change < 0 else "â¡ï¸"
                    
            #         instruments_md += f"### {icon} [{name} ({symbol})]({link})\n\n"
            #         instruments_md += f"- **í˜„ì¬ê°€**: {last_price:,.2f}\n"
            #         instruments_md += f"- **ë³€ë™**: {change:+.2f} ({change_percent:+.2f}%)\n\n"
            
            # 7. ë‚ ì§œ
            pub_date = datetime.now()
            date_str = pub_date.strftime('%Y-%m-%d')

            # 8. íŒŒì¼ëª… ìƒì„±
            filename_base = self.sanitize_filename(title_kr)
            if not filename_base or len(filename_base) < 5:
                filename_base = self.sanitize_filename(original_title)

            filename = f"{date_str}-{filename_base}.md"
            filepath = self.posts_dir / filename

            # 9. ì¤‘ë³µ íŒë‹¨: front matterì˜ article_idë¡œ í™•ì¸
            article_id = article.get('id', '')
            is_duplicate = False

            if filepath.exists():
                # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ front matterì—ì„œ article_id í™•ì¸
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        existing_content = f.read()
                        # front matterì—ì„œ article_id ì¶”ì¶œ
                        import re as regex_module
                        match = regex_module.search(r'article_id:\s*["\']?([^"\'\n]+)["\']?', existing_content)
                        if match:
                            existing_article_id = match.group(1)
                            if existing_article_id == article_id:
                                is_duplicate = True
                except:
                    pass

            if is_duplicate:
                print(f"  [SKIP] ì¤‘ë³µ ê¸°ì‚¬ (ID: {article_id}): {filename}")
                return False
            
            # 10. Jekyll Front Matter ìƒì„±
            image_line = f'image: "{image_url}"\n' if image_url else ''

            # YAML ì´ìŠ¤ì¼€ì´í”„: ì‘ì€ë”°ì˜´í‘œ ì‚¬ìš© (ë” ì•ˆì „)
            title_escaped = title_kr.replace("'", "''")
            excerpt_escaped = excerpt.replace("'", "''")

            # ì£¼ì‹ íƒœê·¸ ìƒì„± (symbolê³¼ instrument_id í¬í•¨)
            stock_tags = []
            if instruments:
                for inst in instruments:
                    symbol = inst.get('symbol', '')
                    inst_id = inst.get('id', '')
                    exchange_id = inst.get('exchange_id', '')
                    if symbol and inst_id:
                        stock_tags.append({
                            'symbol': symbol,
                            'instrument_id': inst_id,
                            'exchange_id': exchange_id
                        })

            # YAML í˜•ì‹ìœ¼ë¡œ stock_tags ìƒì„±
            stock_tags_yaml = ""
            if stock_tags:
                stock_tags_yaml = "stock_tags:\n"
                for tag in stock_tags:
                    stock_tags_yaml += f"  - symbol: {tag['symbol']}\n"
                    stock_tags_yaml += f"    instrument_id: {tag['instrument_id']}\n"
                    if tag['exchange_id']:
                        stock_tags_yaml += f"    exchange_id: {tag['exchange_id']}\n"

            front_matter = f"""---
layout: post
title: '{title_escaped}'
date: {pub_date.strftime('%Y-%m-%d %H:%M:%S +0900')}
categories: [Financial]
author: "Investing.com"
article_id: "{article_id}"
{image_line}excerpt: '{excerpt_escaped}'
{stock_tags_yaml}---

{content_kr}


---
"""

            # 11. íŒŒì¼ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(front_matter)

            print(f"  [OK] í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {filename}\n")
            return True
            
        except Exception as e:
            print(f"  [ERROR] í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run(self, limit=5):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        print("=" * 70)
        print("Investing.com ì™„ì „íŒ í¬ë¡¤ëŸ¬ (í•œêµ­ì–´)")
        print("Breaking News + ì „ì²´ ë³¸ë¬¸ + ì´ë¯¸ì§€ + ì£¼ì‹ ì •ë³´")
        print("=" * 70)
        
        # APIë¡œ Breaking news ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        articles = self.fetch_breaking_news_api()
        
        if not articles:
            print("\n[ERROR] API í˜¸ì¶œ ì‹¤íŒ¨")
            return
        
        print(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬, ìµœëŒ€ {limit}ê°œ ì²˜ë¦¬\n")
        
        # ê° ê¸°ì‚¬ ì²˜ë¦¬
        created_count = 0
        for i, article in enumerate(articles[:limit], 1):
            try:
                if self.create_post(article, i):
                    created_count += 1
            except Exception as e:
                print(f"[ERROR] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        print("\n" + "=" * 70)
        print(f"OK: ì™„ë£Œ - {created_count}ê°œì˜ í¬ìŠ¤íŠ¸ ìƒì„±ë¨")
        print("=" * 70)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Investing.com ì™„ì „íŒ í¬ë¡¤ëŸ¬ (í•œêµ­ì–´)')
    parser.add_argument('--limit', type=int, default=5, help='ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸: 5)')
    args = parser.parse_args()
    
    crawler = InvestingCompleteKR()
    crawler.run(limit=args.limit)


if __name__ == "__main__":
    main()

